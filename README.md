# Assignment 4
Colab: https://drive.google.com/file/d/1OyE2f-UsOA8HKto_UD513dUn-LZbRcb4/view?usp=sharing

We start our implementation by discussing the dataset setup. In this notebook, we will use CIFAR100, which has 100 classes each with 600 images of size  32Ã—32  pixels. Instead of splitting the training, validation and test set over examples, we will split them over classes: we will use 80 classes for training, and 10 for validation and 10 for testing. Our overall goal is to obtain a model that can distinguish between the 10 test classes with seeing very little examples. First, let's load the dataset and visualize some examples.
Next, we need to prepare the dataset in the training, validation and test split as mentioned before. The torchvision package gives us the training and test set as two separate dataset objects. The next code cells will merge the original training and test set, and then create the new train-val-test split.
Now, we can create the class splits. We will assign the classes randomly to training, validation and test, and use a 80%-10%-10% split.
As we can see, the classes have quite some variety and some classes might be easier to distinguish than others.
Finally, we can create the training, validation and test dataset according to our split above. For this, we create dataset objects of our previously defined class ImageDataset.
Finally, to ensure that our implementation of the data sampling process is correct, we can sample a batch and visualize its support and query set. What we would like to see is that the support and query set have the same classes, but distinct examples.
After implementing the model, we can already start training it. We use our common PyTorch Lightning training function, and train the model for 200 epochs. The training function takes model_class as input argument, i.e. the PyTorch Lightning module class that should be trained, since we will reuse this function for other algorithms as well
Our goal of meta-learning is to obtain a model that can quickly adapt to a new task, or in this case, new classes to distinguish between. To test this, we will use our trained ProtoNet and adapt it to the 10 test classes. Thereby, we pick  ğ‘˜  examples per class from which we determine the prototypes, and test the classification accuracy on all other examples. This can be seen as using the  ğ‘˜  examples per class as support set, and the rest of the dataset as a query set. We iterate through the dataset such that each example has been once included in a support set. The average performance over all support sets tells us how well we can expect ProtoNet to perform when seeing only  ğ‘˜  examples per class. During training, we used  ğ‘˜=4 . In testing, we will experiment with  ğ‘˜={2,4,8,16,32}  to get a better sense of how  ğ‘˜  influences the results.
As we initially expected, the performance of ProtoNet indeed increases the more samples we have. However, even with just two samples per class, we classify almost half of the images correctly, which is well above random accuracy (10%). The curve shows an exponentially dampend trend, meaning that adding 2 extra examples to  ğ‘˜=2  has a much higher impact than adding 2 extra samples if we already have  ğ‘˜=16 . Nonetheless, we can say that ProtoNet adapts fairly well to new classes
